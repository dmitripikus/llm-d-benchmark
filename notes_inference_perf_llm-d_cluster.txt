#./run.sh  -d -p e2e-solution2 -t inference-gateway -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim -m 'meta-llama/Llama-3.1-70B-Instruct' -w apps_input -l fmperf -c $(realpath ../gil.sh)

./run.sh  -p e2e-solution2 -t inference-gateway -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim -m 'meta-llama/Llama-3.1-70B-Instruct' -w apps_input -l fmperf -c $(realpath ./gilenv.sh)

brew autoremove


./run.sh  -p e2e-solution2 -t inference-gateway -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim -m 'meta-llama/Llama-3.1-70B-Instruct' -w sanity_short-input -l fmperf -c $(realpath ./gilenv.sh)

./run.sh  -p e2e-solution2 -t inference-gateway -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim -m 'meta-llama/Llama-3.1-70B-Instruct' -w medium_model_long_input -l fmperf -c $(realpath ./gilenv.sh)

+++++++

 -s 1000000


inference-perf!!!!!
==============

./run.sh \
    -p llm-d-inference-scheduling \
    -t infra-inference-scheduling-inference-gateway \
    -k workload-pvc \
    -m 'meta-llama/Llama-3.1-70B-Instruct' \
    -w workload_chatbot_sharegpt.yaml \
    -l inference-perf \
    -c $(realpath ./inf_perf_env.sh) \
    -s 1000000


./run.sh \
    -p e2e-solution2 \
    -t inference-gateway \
    -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim \
    -m 'meta-llama/Llama-3.1-70B-Instruct' \
    -w workload_sched_synthetic_short.yaml \
    -l inference-perf \
    -c $(realpath ./inf_perf_env.sh) \
    -s 1000000

./run.sh \
    -p e2e-solution2 \
    -t inference-gateway \
    -k vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim \
    -m 'meta-llama/Llama-3.1-70B-Instruct' \
    -w workload_shared_prefix_synthetic_short.yaml \
    -l inference-perf \
    -c $(realpath ./inf_perf_env.sh) \
    -s 1000000








./e2e.sh \
    -p haifa-benchmark \
    -t modelservice \
    -m 'meta-llama/Llama-3.1-70B-Instruct' \
    -w workload_shared_prefix_synthetic_exp1 \
    -l inference-perf \
    -s 4

./e2e.sh \
    -p haifa-benchmark \
    -t standalone \
    -m 'meta-llama/Llama-3.1-70B-Instruct' \
    -w workload_shared_prefix_synthetic_exp1 \
    -l inference-perf

++++++++++++++

["-p", "haifa-benchmark",
 "-t", "modelservice",
 "-m", "'meta-llama/Llama-3.1-70B-Instruct'",
 "-w", "workload_shared_prefix_synthetic_exp1",
 "-l", "inference-perf"
 ]

ERROR while executing command "oc --namespace e2e-solution2 cp access-to-harness-data-vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim:/requests/inference-perf_1753469519_inference-gateway-70b-instruct /Users/dpikus/DP_FILES/Data/projects/Inf_Router/repos/dev_bench_inf_perf/results/inference-perf_1753469519_inference-gateway-70b-instruct"

Error from server (BadRequest): pod access-to-harness-data-vllm-p2p-70b-chart-llama-3-70b-instruct-storage-claim does not have a host assigned



kubectl -n e2e-solution2 scale deployment dima-hash-fix-vllm-llama-3-70b-instruct --replicas 4




k get cm epp-config -o yaml | yq '.data["epp-config.yaml"]'


quay.io/dpikus/lm-benchmark:upstream

qps_values: "2 5 8 10 13 17 20"

2025-07-31 10:34:34,642 - inference_perf.client.filestorage.local - INFO - Report files will be stored at: /requests/inference-perf_1753958017_inference-gateway-70b-instruct

