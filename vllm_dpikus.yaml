apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "3"
    meta.helm.sh/release-name: ms-inference-scheduling
    meta.helm.sh/release-namespace: dpikus-ns
  creationTimestamp: "2025-09-30T14:37:19Z"
  generation: 6
  labels:
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/version: v0.2.0
    helm.sh/chart: llm-d-modelservice-v0.2.11
  name: ms-inference-scheduling-llm-d-modelservice-decode
  namespace: dpikus-ns
  resourceVersion: "106625645"
  uid: 719d3ea9-6c25-4923-a072-cea47ed1931c
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      llm-d.ai/inferenceServing: "true"
      llm-d.ai/model: ms-inference-scheduling-llm-d-modelservice
      llm-d.ai/role: decode
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        llm-d.ai/inferenceServing: "true"
        llm-d.ai/model: ms-inference-scheduling-llm-d-modelservice
        llm-d.ai/role: decode
    spec:
      containers:
      - args:
        - Qwen/Qwen3-1.7B
        - --port
        - "8200"
        - --served-model-name
        - Qwen/Qwen3-1.7B
        - --enforce-eager
        - --kv-transfer-config
        - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
        command:
        - vllm
        - serve
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: UCX_TLS
          value: cuda_ipc,cuda_copy,tcp
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: status.podIP
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "5557"
        - name: VLLM_LOGGING_LEVEL
          value: DEBUG
        - name: DP_SIZE
          value: "1"
        - name: TP_SIZE
          value: "1"
        - name: HF_HOME
          value: /model-cache
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: HF_TOKEN
              name: llm-d-hf-token
        image: ghcr.io/llm-d/llm-d-dev:pr-245@sha256:06ba310cd557b7762c391ac11fe0bf20307851769bcf42973608ddcc3b4b47cf
        imagePullPolicy: IfNotPresent
        name: vllm
        ports:
        - containerPort: 5557
          protocol: TCP
        - containerPort: 8200
          name: metrics
          protocol: TCP
        resources:
          limits:
            nvidia.com/gpu: "1"
          requests:
            nvidia.com/gpu: "1"
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /.config
          name: metrics-volume
        - mountPath: /.cache
          name: torch-compile-cache
        - mountPath: /model-cache
          name: model-storage
      dnsPolicy: ClusterFirst
      initContainers:
      - args:
        - --port=8000
        - --vllm-port=8200
        - --connector=nixlv2
        - -v=5
        - --secure-proxy=false
        image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.3.0
        imagePullPolicy: Always
        name: routing-proxy
        ports:
        - containerPort: 8000
          protocol: TCP
        resources: {}
        restartPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: ms-inference-scheduling-llm-d-modelservice
      serviceAccountName: ms-inference-scheduling-llm-d-modelservice
      terminationGracePeriodSeconds: 30
      volumes:
      - emptyDir: {}
        name: metrics-volume
      - emptyDir: {}
        name: torch-compile-cache
      - emptyDir:
          sizeLimit: 20Gi
        name: model-storage
status:
  conditions:
  - lastTransitionTime: "2025-09-30T14:37:19Z"
    lastUpdateTime: "2025-10-30T16:17:31Z"
    message: ReplicaSet "ms-inference-scheduling-llm-d-modelservice-decode-79fb55b66b"
      has successfully progressed.
    reason: NewReplicaSetAvailable
    status: "True"
    type: Progressing
  - lastTransitionTime: "2025-10-30T16:30:27Z"
    lastUpdateTime: "2025-10-30T16:30:27Z"
    message: Deployment does not have minimum availability.
    reason: MinimumReplicasUnavailable
    status: "False"
    type: Available
  observedGeneration: 6
  replicas: 1
  unavailableReplicas: 1
  updatedReplicas: 1
